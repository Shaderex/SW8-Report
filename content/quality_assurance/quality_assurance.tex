%!TEX root = ../../super_main.tex

\chapter{Quality Assurance}
\label{cha:quality_assurance}

This section describes the different measures we have used in order ensure the quality of our developed product 
\\\\
\todo{Write more intro to QA}. 
When we executed tests that required that we run the code om some device, we always attempted to run it on multiple different devices in order to support several different hardware configurations and also that we are compatible with multiple Android versions. The phones we have available all run stock Android versions from the Android open source project. This might indicate that we also cover additional OS versions that are based on the stock versions, since if we find some bug when we test against the stock version, it is most likely also present in the modified version. 

\input{content/quality_assurance/continuous_integration}

\input{content/quality_assurance/automated_unit_test}

\section{Monkey Test}
\label{sec:monkey_test}
We executed UI/Application Exerciser Monkey tests on the android code. The exerciser monkey generates pseudo-random streams of user events, which can be used to test the robustness of the application. The monkey is able to stress-test the application because of frequent button clicks, etc., such that it most likely crashes if there are memory leaks or other bad implementations. It furthermore provides the possibility of simulating erratic user behavior, that might perform some trace of actions that human users would not typically follow, which might crash the application. Whenever the monkey crashes the application, the entire trace is available, but we mainly used it for the crash reports and exceptions that are available through the Android Debug Bridge. 
\\\\
We started by running one trace on 50000 inputs on a Galaxy Nexus phone, where the application crashed after 46000 actions, which allowed us to take a look at the exception and resolve it. Afterwards the monkey ran for 2 hours straight without crashing, and we therefore think the application is rather robust.
\\\\
Following this we also ran the monkey for a while on a Nexus 5 device which had Android 6.x, in contrast to the 5.x on the Galaxy Nexus. Here it also seemed to run without any difficulties, which gives some indication that our application is resilient in terms of different configurations and also compatible with different versions of Android without issues.
\\\\
We have also been using the monkey to discover cases where, if you executed a certain set of actions fast enough, it would would result in incorrect application behavior. This includes things such as subscribing to a campaign, immediately exiting the menu and then entering the same menu again, where the communication between application and server had not finished registering the participant yet. This allows us to find certain use patterns that we need to consider, or at least need to keep in mind, even if we do not have solutions for them at the time.
\\\\
It is also possible to configure CI servers to run automated monkey tests if so desired, but we did not think this was a good idea for our project. This was partially because it takes quite a bit of time to execute the monkey test, but also because it has access to the settings of the phone, which might mess up the unit tests. So given the amount of precautions, resets, etc., that might be necessary, we deemed it to take too much time to set up. 

\section{Pair Group Review}
\label{sec:pair_group_review}

Two times during the project period we arranged meetings with another software student group at Aalborg University, where we presented the current states of our projects and provided critique for each other. One meeting was arranged halfway through the semester and another was arranged three weeks before project delivery. 

\subsection{First Meeting: Project Idea}
\label{sub:first_meeting_project_idea}
The goal of the first meeting was to evaluate the general project idea of the opponent group and also suggest improvements for the developed system. At this meeting the two groups were unfamiliar with the product of the other group, so an agenda was created that should increase the understanding for the opponent:

\begin{itemize}
    \item Introduction
    \item Demonstration
    \item Evaluation
\end{itemize}

We took turns in presenting, so the first group started by introducing their problem area, how they were going to solve the problem and which customers might be interesting in using the developed product. After the presentation of the problem area, the group presented the current state of the product, which was followed by two different evaluations from the opponent group. One evaluation was regarding the idea and the marketability of the solution, while the other was regarding how the product could be improved. 
\\\\
The result of this meeting were mainly on the idea level, and not so much about the product specifically. Our opponent group had concerns regarding how we were going to persuade participants to use the system, such that we could actually gather the data that we claimed would be available for our customers. We decided that this was not a big concern, since the customers would be interested in branding their campaigns in order to gather participants, or might offer rewards for the participants. In any case, we think that fact that we provide a platform for customers to use, so that they only have to provide some incentive for participants to join, is an improvement in itself. We furthermore hope that the participants who join for the rewards of the branded campaigns would be interested in selecting campaigns that only offered small or no rewards. We think it is realistic to assume, that if customers do not offer any rewards whatsoever on their own without our system, they would probably not get any participants anyway. 

\subsection{Second Meeting: Product Evaluation}
\label{sub:second_meeting_product_evaluation}

At the second meeting, both the groups' products were in an almost finished state, and were presented for the other group. The goal of this meeting was to determine if there were any obscure parts of the system, bugs, possible usability improvements or extensions to the system. Our opponent group could not find any bugs in the system, but they found some usability improvements and an extension that would be nice to have in the system. The suggestions are represented in \tabref{tab:suggestions_from_opponent_group}. 

\begin{table}[!htbp]
    \centering
    \begin{tabular}{|l|p{0.8\textwidth}|}
    \hline
    \textbf{Category} & \textbf{Description} \\ \hline
    Extension & Run questionnaires at specific times of the day, instead of relative to time of joining campaign.  \\ \hline
    Usability & Put the definition of our time intervals on the website, or show it with an image. \\ \hline
    Usability & Move the active campaign to the top of the campaigns list instead of where it was located before. \\ \hline
    Usability & Have focus on the size differences on the website. Many items are rather small, so it is hard to determine what is important. The most important things should be large. \\ \hline
    \end{tabular}
    \caption{Suggestions from opponent group.}
    \label{tab:suggestions_from_opponent_group}
\end{table}

These were all valid suggestions, but due to the meeting being held a bit too close to the project deadline, we had to down prioritize the development. Effectively, this resulted in only the second entry being implemented: \emph{Put the definition of our time intervals on the website, or show it with an image}. There is now an image on the website which represents how to understand the different intervals you can specify on the ``create campaign''-site. The other three entries are things that would be nice improvements to the system, and definitely something that should be worked on, if the system was to be extended at some point. 



