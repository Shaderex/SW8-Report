%!TEX root = ../../super_main.tex

\chapter{Quality Assurance}
\label{cha:quality_assurance}

This section describes the different measures we have used in order ensure the quality of our developed product 
\\\\
\todo{Write more intro to QA}. 
When we executed tests that required that we run the code om some device, we always attempted to run it on multiple different devices in order to support several different hardware configurations and also that we are compatible with multiple Android versions. The phones we have available all run stock Android versions from the Android open source project. This might indicate that we also cover additional OS versions that are based on the stock versions, since if we find some bug when we test against the stock version, it is most likely also present in the modified version. 


\todo{Overvej at lave pair programming section}

\section{Automated Unit Test}
\label{sec:automated_unit_test}

Our development method states that all code we develop should be made in a test-first fashion (see \secref{sec:extreme_programming}). We therefore attempted to always make unit tests for all the features we implemented. In some cases it was not possible to create dynamic white box unit tests, and we therefore, in these cases, switched to a dynamic black box approach, where we made test specifications, executed some part of the code manually and observed the result relative to the specification. This was mainly for our integration tests, which means our high level constructs/designs do not have any unit tests. This results in a code coverage from the unit tests that are lower than expected, but we can confirm with manual tests that the behavior is as expected. 
\\\\
Our code coverage graphs can be seen in \figref{android_project_code_coverage} and \figref{php_project_code_coverage}, for our Android and PHP projects respectively. We have not used a more strict testing metric than line coverage, because we did not have the time to spend on it. We can therefore not guarantee that all things such as branches, exceptions, etc, are tested in the code, but we can guarantee that it is possible to run most of our code successfully. In the android project our line coverage percentage is 43\%, which is rather low. But as explained previously, this is due to the complexity of the integration tests which were manual instead. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{graphic/quality_assurance/jenkins_android_code_coverage}
    \caption{Android project code coverage}
    \label{fig:android_project_code_coverage}
\end{figure}

\noindent
In the PHP project, we have a 74\% line coverage, which is rather good. Most of the untested code is library- or autogenerated code, and we have not tested this because we assume these parts work as they are supposed to. This is a risk management assessment we have made, and deemed insignificant, in contrast to the speed we gain from using the libraries without testing them. If we only considered coverage on the code we made ourselves, it would probably exceed 90\%. 
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{graphic/quality_assurance/jenkins_php_code_coverage}
    \caption{PHP project code coverage}
    \label{fig:php_project_code_coverage}
\end{figure}

\section{Continuous Integration}
\label{sec:continuous_integration}
As mentioned in \secref{sec:extreme_programming}, we wanted a continuous integration server in order to ensure that our code base always was at a stable state. We installed Jenkins on the same server that makes up the server part of our client-server architecture because it was easily available. Jenkins is an open source automation server, which supports various different plugins that helps with builds, viewing test results, etc. We configured this Jenkins server to be notified whenever our Github version control code repositories for both Android and PHP code were changed. When this notification happened, Jenkins would build the corresponding project and automatically run its unit tests. Whenever the build projects would go from a previously successful build to a now failing build or vice versa, the Jenkins system would send out mails to our group, so we were aware that something went wrong or that it was now fixed again. This made it possible for us to give immediate attention to issues that we did not catch before pushing our content to the version control. See \figref{fig:jenkins_front_page} for an illustration of how the front page of Jenkins. In the left side, the build queue can be seen, which shows if any builds are currently running. In the center, the various projects that are currently configured for the Jenkins setup can be seen. The blue circle changes to red if the most recent build was a failure, and besides that, it can be seen how long time has passed since the most recent pass, most recent failure, and how long the last build took to finish. By clicking on one of the projects, statistics can be seen for checkstyes errors, code coverage, and test results. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{graphic/quality_assurance/jenkins_frontpage.png}
    \caption{Jenkins CI front page}
    \label{fig:jenkins_front_page}
\end{figure}
\FloatBarrier

We used this extensively during the development, because when several people merge their work into the master branch of the version control several times per day, something is bound to go wrong eventually. This ensured that we always knew if something was wrong with either of our projects, so we knew when we needed to allocate people for fixing it. 

\section{Monkey Test}
\label{sec:monkey_test}
We executed UI/Application Exerciser Monkey tests on the android code. The exerciser monkey generates pseudo-random streams of user events, which can be used to test the robustness of the application. The monkey is able to stress-test the application because of frequent button clicks, etc., such that it most likely crashes if there are memory leaks or other bad implementations. It furthermore provides the possibility of simulating erratic user behavior, that might perform some trace of actions that human users would not typically follow, which might crash the application. Whenever the monkey crashes the application, the entire trace is available, but we mainly used it for the crash reports and exceptions that are available through the Android Debug Bridge. 
\\\\
We started by running one trace on 50000 inputs on a Galaxy Nexus phone, where the application crashed after 46000 actions, which allowed us to take a look at the exception and resolve it. Afterwards the monkey ran for 2 hours straight without crashing, and we therefore think the application is rather robust.
\\\\
Following this we also ran the monkey for a while on a Nexus 5 device which had Android 6.x, in contrast to the 5.x on the Galaxy Nexus. Here it also seemed to run without any difficulties, which gives some indication that our application is resilient in terms of different configurations and also compatible with different versions of Android without issues.
\\\\
We've also been using the monkey to discover cases where, if you executed a certain set of actions fast enough, it would would result in incorrect app behavior. This includes things such as subscribing to a campaign, immediately exiting the menu and then entering the same menu again, where the communication between app and server had not completed to register the participant yet. This allows us to find certain use patterns that we need to consider, or at least need to keep in mind, even if we don't have solutions for them at the time.
\\\\
It is also possible to configure CI servers to run automated monkey tests if so desired, but we did not think this was a good idea for our project. This was partially because it takes quite a bit of time to execute the monkey test, but also because it has access to the settings of the phone, which might mess up the unit tests. So given the amount of precautions, resets, etc., that might be necessary, we deemed it to take too much time to set up. 

\section{Pair Group Review}
\label{sec:pair_group_review}

Two times during the project period we arranged meetings with another software student group at Aalborg University, where we presented the current states of our projects and provided critique for each other. One meeting was arranged halfway through the semester and another was arranged three weeks before project delivery. 

\subsection{First Meeting: Project idea}
\label{sub:first_meeting_project_idea}
The goal of the first meeting was to evaluate the general project idea of the opponent group and also suggest improvements for the developed system. At this meeting the two groups were unfamiliar with the product of the other group, so an agenda was created that should increase the understanding for the opponent:

\begin{itemize}
    \item Introduction
    \item Demonstration
    \item Evaluation
\end{itemize}

We took turns in presenting, so the first group started by introducing their problem area, how they were going to solve the problem and which customers might be interesting in using the developed product. After the presentation of the problem area, the group presented the current state of the product, which was followed by two different evaluations from the opponent group. One evaluation was regarding the idea and the marketability of the solution, while the other was regarding how the product could be improved. 
\\\\
The result of this meeting were mainly on the idea level, and not so much about the product specifically. Our opponent group had concerns regarding how we were going to persuade participants to use the system, such that we could actually gather the data that we claimed would be available for our customers. We decided that this was not a big concern, since the customers would be interested in branding their campaigns in order to gather participants, or might offer rewards for the participants. In any case, we think that fact that we provide a platform for customers to use, so that they only have to provide some incentive for participants to join, is an improvement in itself. We furthermore hope that the participants who join for the rewards of the branded campaigns would be interested in selecting campaigns that only offered small or no rewards. We think it is realistic to assume, that if customers do not offer any rewards whatsoever on their own without our system, they would probably not get any participants anyway. 


\subsection{Second Meeting: Product evaluation}
\label{sub:second_meeting_product_evaluation}

At the second meeting, both the groups' products were in an almost finished state, and were presented for the other group. The goal of this meeting was to determine if there were any obscure parts of the system, bugs, possible usability improvements or extensions to the system. Our opponent group could not find any bugs in the system, but they found some usability improvements and an extension that would be nice to have in the system. The suggestions are represented in the following list: 

\begin{table}[!htbp]
    
    \centering
    \begin{tabular}{|l|p{0.8\textwidth}|}
    \hline
    \textbf{Category} & \textbf{Description} \\ \hline
    Extension & Run questionnaires at specific times of the day, instead of relative to time of joining campaign.  \\ \hline
    Usability & Put the definition of our time intervals on the website, or show it with an image. \\ \hline
    Usability & Move the active campaign to the top of the campaigns list instead of where it was located before. \\ \hline
    Usability & Have focus on the size differences on the website. Many items are rather small, so it is hard to determine what is important. The most important things should be large. \\ \hline
    \end{tabular}
    \caption{Suggestions from opponent group.}
    \label{tab:suggestions_from_opponent_group}
\end{table}

These were all valid suggestions, but due to the meeting being held a bit too close to the project deadline, we had to down prioritize the development. Effectively, this resulted in only the second entry being implemented: Put the definition of our time intervals on the website, or show it with an image. There is now an image on the website which represents how to understand the different intervals you can specify on the ``create campaign'' website. The other three bullets are things that would be nice improvements to the system, and definitely something that should be worked on, if the system was to be extended at some point. 



