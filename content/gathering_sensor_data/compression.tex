%!TEX root = ../../super_main.tex

\section{Compression}
\label{sec:compression}

As described in \secref{sec:data_quantity_estimation}, we would like keep the power consumption of our application low. One way of doing this is ensuring that the amount of data sent from our application to our server is the minimum amount necessary, because sending less data costs less power. During the development we discovered that many of the data types used for sensor measurements have significant overhead, compared to what is necessary in order to store the data, which opens up for the possibility of compressing the data. 
\\\\
An example of this is the heart rate sensor in the Microsoft Band 2 wearable, which uses an signed integer in order to store a heart rate. A signed integer is typically 32 bits, and has a maximum value of $2^{31} - 1$ (2,147,483,647), while a heart rate over 250 is very abnormal. This knowledge would allow us to exploit the limited value range, and use as little as 8-10 bits (values 255-1023). This would effectively cut off the majority of bits used to store a value. Furthermore, Microsoft Band 2 also uses a boolean to represent if the reading is precise (they call it locked), which uses 8 bits. This effectively means that even if you use 10 bits for the heart rate, and 1 for the locked state, you would still save 29 bits.    
\\\\
There are other similar cases, and we decided to implement a strategy for compressing one of the common measurements returned by many sensors, namely an array of floats containing three values. An example of a sensor that returns such measurements is the accelerometer, which measures how the phone moves on its $x$, $y$, and $z$ axes. We designed a class called \mono{FloatTripleMeasurement} which is used to compress these three values to a single \mono{long}, meaning we convert $12$ bytes ($3$ floats $\times 4$ bytes) to be stored in $8$ bytes ($\sim 33\%$ space reduction). We do this by sacrificing some of the precision of a regular float, because we think the regular precision is overkill, both in terms of sensor inaccuracy and also its usefulness for the customers. When compressing, we strip the decimal separator from the floats and treat them as a $20$ bit integer value, where the first bit determines whether the value is positive or negative. This means that we now use $60$ bits for storing the compressed value of the floats. We use the remainder of the long to add 3 bits for representing where the decimal should be placed, i.e. how any decimals we include. A visualization of the method used to compress floats can be seen in \figref{fig:float_triple_convert} and \figref{fig:float_triple_bit} respectively.

\begin{figure}[!htbp]
    \begin{alignat*}{6}
       &8.138   &&                   &&   && 8138  &&                   && \text{\mono{00000001111111001010}} \\
      -&12.821  && ~~ \rightarrow ~~ && - && 12821 && ~~ \rightarrow ~~ && \text{\mono{10000011001000010101}} \\
       &42.4878 &&                   &&   && 42487 &&                   && \text{\mono{00001010010111110111}} 
    \end{alignat*}
    \caption{Conversion of floats to binary representation in \mono{FloatTripleMeasurement}.}
    \label{fig:float_triple_convert}
\end{figure}
\FloatBarrier

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{graphic/gathering_sensor_data/float_triple_bit}
    \caption{The bit representation of a \mono{FloatTripleMeasurement}.}
    \label{fig:float_triple_bit}
\end{figure}
\FloatBarrier

We wanted to compress the floating point values of the sensors so that the size of the data we need to send over network and store on the device would decrease. However, this compression does not come for free. We actively make a trade off between size of data stored and CPU cycles used for compressing and decompressing the values. We have conducted a performance test for our compression of floats that investigates how much extra time has to be spent on compressing and decompressing the floats. 
\\\\
The test was conducted by generating three pseudo random floating point numbers (from $0$ to $1$). These three numbers would then be compressed by the mechanism described previously. This was repeated for a specific amount of compressions. All of the tests were evaluated $1,000$ times, to avoid measuring different noise factors such as CPU starvation. The tests are performed on two different devices: a Nexus 5 and a OnePlus One. Ideally the test should be performed on several different devices on different tiers, however, we do not have access to a big variety than of devices, and have thus only tested on two. The results of the tests can be seen in \figref{fig:result_of_compression_test}. The blue dots are tests performed on the Nexus device, while red dots are tests from the OnePlus One device.

% \begin{filecontents}{nexus.data}
% 1 6
% 2 12
% 3 19
% 4 23
% 5 31
% 6 38
% 7 34
% 8 44
% 9 54
% 10 64
% 11 70
% 12 57
% 13 71
% 14 89
% 15 91
% 16 89
% 17 108
% 18 112
% 19 109
% 20 124
% 21 121
% 22 124
% 23 130
% 24 140
% 25 149
% \end{filecontents}

% \begin{filecontents}{oneplus.data}
% 1 3
% 2 7
% 3 9
% 4 14
% 5 17
% 6 19
% 7 24
% 8 27
% 9 32
% 10 34
% 11 34
% 12 43
% 13 46
% 14 49
% 15 54
% 16 54
% 17 61
% 18 56
% 19 67
% 20 63
% 21 75
% 22 77
% 23 83
% 24 74
% 25 78
% \end{filecontents}

\begin{figure}[!htbp]
    \centering

    \begin{tikzpicture}[y=.05cm, x=.4cm,font=\sffamily]
        %axis
        \draw (0,0) -- coordinate (x axis mid) (25,0);
            \draw (0,0) -- coordinate (y axis mid) (0,150);
            %ticks
            \foreach \x in {1,3,...,25}
                \draw (\x,1pt) -- (\x,-3pt)
                node[anchor=north] {\x};

            \foreach \y in {0,15,...,150}
                \draw (1pt,\y) -- (-3pt,\y) 
                    node[anchor=east] {\y}; 
        %labels      
        \node[below=0.8cm] at (x axis mid) {Amount of thousand float triples compressed};
        \node[rotate=90, above=10cm, left=1.2cm] at (y axis mid) {Time used [ms]};

        \draw plot[mark=*, mark options={fill=blue}] file {content/gathering_sensor_data/nexus.data};

        \draw plot[mark=*, mark options={fill=red}] file {content/gathering_sensor_data/oneplus.data};

    \end{tikzpicture}

    \caption{Results of compression test.}
    \label{fig:result_of_compression_test}
\end{figure}
\FloatBarrier

As can be seen in \tabref{fig:result_of_compression_test}, there is a somewhat linear correlation between the amount of floats compressed, and the time it takes to compress them. Furthermore, there should not be a problem in regards to the time required to make the compression, since the method is able to compress roughly $\sim 167$ float triples every ms and the application will probably never output this many measurements this frequently. With this in mind, one has to consider if this additional time (extra CPU cycles and extra battery usage), is worth the tradeoff. On one hand, it is possible to spend additional CPU cycles, and thereby power, before storing data on the device, in order to save $\sim 33\%$ of the space, while having to transfer less data to the server and thereby spending less power and CPU cycles on this. On the other hand we could save all the power before storing, just save the normal floats in the database, and then have to spend extra power for transferring (because there is simply more data). We do not know if our approach uses more power than simply not doing it, but if the system should be expanded into something large scale, it should definitely be tested. One thing that we can definitely know for sure is that our approach is more space efficient. 

\subsection{General Compression}
\label{sub:general_compression}

There are furthermore the considerations of how compression algorithms will handle our \mono{FloatTripleMeasurement} (long) compared to the three floats. The library we use for HTTP-communication has the possibility of automatically compressing requests to a format called gzip\footnote{http://www.gzip.org/}. To see if this compression would further reduce the space of the collection information, we conducted a small test, where we compressed (using gzip) some snapshots containing \mono{FloatTripleMeasurement}s, and compared them to compressed (also using gzip) snapshots containing regular floats. Note that both the snapshots were serialized using JSON, similar to what is outputted by the server, as seen in \lstref{lst:snapshot_json_example}. When compression with the gzip program, you may specify how fast the algorithm should perform the compression. The faster you specify, the less compression would be achieved. We conducted a test where we would compress using the \emph{fast} setting, but also the \emph{best} setting (slowest). The result of the conducted test can be seen in \tabref{tab:gzip_compression}. All sizes are measured in bytes.

\begin{table}[!htbp]
\centering
\begin{tabular}{r|l|l|l|}
\cline{2-4}
                                                                           & \textbf{No gzip} & \textbf{Fast gzip} & \textbf{Best gzip} \\ \hline
\multicolumn{1}{|r|}{\textbf{Size with \mono{FloatTripleMeasurements}}}    & 101,543          & 40,532             & 38,358             \\ \hline
\multicolumn{1}{|r|}{\textbf{Size with regular float measurements}}        & 118,660          & 44,893             & 38,643             \\ \hline
\multicolumn{1}{|r|}{\textbf{Difference}}                                  & 17,117           & 4,361              & 285                \\ \hline
\end{tabular}
\caption{gzip float compression test result. All sizes are in bytes.}
\label{tab:gzip_compression}
\end{table}
\FloatBarrier

This test indicates, as expected, that the biggest size difference is when not using gzip. However, when using the \mono{fast} setting, gzip will further compress the data to $\sim 38-40\%$ of the original size, and here we still see a $\sim 10\%$ difference in the size of two snapshots, indicating that using both compression methods would yield a noticeable result. When using the \mono{best} setting, the data is compressed to $\sim 33-38\%$ of the original size, however, here there is not a big difference ($\sim 1\%$) between the two snapshots, indicating that if we use the \mono{best} setting, we would not gain much in term of size. Keep in mind that this test only contains \mono{FloatTripleMeasurement}s, and might yield a much better result, when using other uncompressed measurements such as simple integers, which are used for heart rate measurements. This effectively means that using gzip is a good idea for minimizing the network impact that our solution have. We have not implemented the use of gzip in the current implementation of the system due to prioritization of more prominent features. It is also important to keep in mind that the knowledge we have regarding the measurements will allow us to make compression methods that potentially are faster (in terms of CPU cycles) compared to gzip, which first need to find these patterns in the data. 

\subsection{Data Transfer Format}
The compression methods, described above, are limited by the data transfer format that we use, namely JSON. By utilizing this format, we convert simple types such as \mono{float}s into their string representation. This means that an otherwise small representation using 4 bytes in Java, could when serialized potentially have the size of the number of characters in the representation multiplied by the size of one character (in ASCII 1 byte). For instance the number -32.456 will in its floating-point representation be 4 bytes, whereas in its string representation it will be 7 bytes (minus and decimal separator included). As mentioned in \secref{sub:background_sensor_service_snapshot_generation_and_upload} it would preferable to invent our own transfer format, which is optimized for the structure of our data. 
% It is therefore important to take into consideration when deciding whether or not to use our own compression, that the transfer gain is rather low. We use knowledge about the data we collect, while gzip utilizes patterns, and we do not know if there there is much to gain if we use this for all the different types of data we gather. There are furthermore the considerations about how fast we can compress our smaller data types compared to the normal implementation, which directly translates into the amount of necessary CPU cycles and thereby also power, which should therefore be checked before making a final decision about which solution is best. As a temporary solution, until the proper analysis has been made, we have decided to not use gzip, because it would too long time to implement on the server side without the guarantee that it would be better, and because we always transfer data on Wi-Fi anyway. 
